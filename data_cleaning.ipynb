{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Australian Freight Export Data Cleaning\n",
    "##### Data Cleaning and Preprocessing for 2024-2025 Export Data\n",
    "\n",
    "This notebook focuses on cleaning and preprocessing the ABS export data before analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark (run this first!)\n",
    "%pip install pyspark\n",
    "\n",
    "# Check if Java is installed (required for Spark)\n",
    "!java -version\n",
    "\n",
    "# Check Python packages\n",
    "!pip list | grep spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set display options for better viewing\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check original product descriptions BEFORE any processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the export data\n",
    "df = pd.read_csv('data/exports_2024_2025.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Initial Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n=== COLUMN INFORMATION ===\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=== MISSING VALUES ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "# Check if any missing values exist\n",
    "if missing_data.sum() == 0:\n",
    "    print(\"No missing values found!\")\n",
    "else:\n",
    "    print(missing_data)\n",
    "    print(missing_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Cleaning and Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric columns and handle missing values\n",
    "print(\"=== CLEANING DATA ===\")\n",
    "\n",
    "# Rename columns for clarity\n",
    "df = df.rename(columns={\n",
    "    'sitc': 'product_description',\n",
    "    'sitc_code': 'prod_descpt_code'\n",
    "})\n",
    "print(\"Renamed 'sitc' to 'product_description' and 'sitc_code' to 'prod_descpt_code'\")\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_columns = ['quantity', 'gross_weight_tonnes', 'value_fob_aud']\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"Converting {col} to numeric...\")\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Fill missing numeric values with 0\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"  Filling {missing_count:,} missing values with 0\")\n",
    "            df[col] = df[col].fillna(0)\n",
    "        \n",
    "        # Convert negative values to 0\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"  Converting {negative_count:,} negative values to 0\")\n",
    "            df[col] = df[col].clip(lower=0)\n",
    "\n",
    "# Fill missing text values\n",
    "text_columns = ['country_of_destination', 'product_description']\n",
    "for col in text_columns:\n",
    "    if col in df.columns:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"Filling {missing_count:,} missing values in {col} with 'Unknown'\")\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "print(\"Data cleaning completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect product descriptions\n",
    "print(\"=== SAMPLE PRODUCT DESCRIPTIONS ===\")\n",
    "print(df['product_description'].head(10))\n",
    "print(f\"\\n=== STATISTICS ===\")\n",
    "print(f\"Unique products: {df['product_description'].nunique()}\")\n",
    "print(f\"Max length: {df['product_description'].str.len().max()}\")\n",
    "print(f\"Min length: {df['product_description'].str.len().min()}\")\n",
    "\n",
    "# Check for whitespace issues (define pattern outside f-string to avoid backslash error)\n",
    "extra_whitespace_count = df['product_description'].str.contains(r'\\s{2,}', regex=True).sum()\n",
    "trailing_space_count = (df['product_description'] != df['product_description'].str.strip()).sum()\n",
    "\n",
    "print(f\"\\nAny with extra whitespace: {extra_whitespace_count}\")\n",
    "print(f\"Any with leading/trailing spaces: {trailing_space_count}\")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean product descriptions\n",
    "print(\"\\n=== CLEANING PRODUCT DESCRIPTIONS ===\")\n",
    "\n",
    "# Strip leading/trailing whitespace\n",
    "df['product_description'] = df['product_description'].str.strip()\n",
    "\n",
    "# Replace multiple spaces with single space\n",
    "df['product_description'] = df['product_description'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Remove line breaks and tabs\n",
    "df['product_description'] = df['product_description'].str.replace(r'[\\n\\r\\t]+', ' ', regex=True)\n",
    "\n",
    "# Standardize quotation marks\n",
    "df['product_description'] = df['product_description'].str.replace('\"', '', regex=False)\n",
    "df['product_description'] = df['product_description'].str.replace(\"'\", '', regex=False)\n",
    "\n",
    "# Optional: Title case for consistency (comment out if you prefer original casing)\n",
    "# df['product_description'] = df['product_description'].str.title()\n",
    "\n",
    "print(f\"Product descriptions cleaned!\")\n",
    "print(f\"Unique products: {df['product_description'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization section removed - now handled by SITC mapping\n",
    "print(\"âœ… Standardization removed - SITC mapping will handle unclassified products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for whitespace issues (define pattern outside f-string to avoid backslash error)\n",
    "extra_whitespace_count = df['product_description'].str.contains(r'\\s{2,}', regex=True).sum()\n",
    "trailing_space_count = (df['product_description'] != df['product_description'].str.strip()).sum()\n",
    "\n",
    "print(f\"\\nAny with extra whitespace: {extra_whitespace_count}\")\n",
    "print(f\"Any with leading/trailing spaces: {trailing_space_count}\")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.5 Derived Features & Transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section creates derived features before final duplicate removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month into separate columns\n",
    "print(\"\\n=== CREATING DERIVED FEATURES ===\")\n",
    "\n",
    "# The original month column format is \"Month_Name YYYY\" (e.g., \"August 2024\")\n",
    "# Extract year (4 digits at the end)\n",
    "df['year'] = df['month'].str.extract(r'(\\d{4})')[0]\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "\n",
    "# Extract month name (everything before the year) - get first column as Series\n",
    "month_name_temp = df['month'].str.extract(r'^([A-Za-z]+)')[0]\n",
    "\n",
    "# Convert month name to month number\n",
    "month_to_number = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4, \n",
    "    'May': 5, 'June': 6, 'July': 7, 'August': 8,\n",
    "    'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "}\n",
    "df['month_number'] = month_name_temp.map(month_to_number)\n",
    "\n",
    "# Replace the original month column with just the month name\n",
    "df['month'] = month_name_temp\n",
    "\n",
    "# Create value per tonne metric\n",
    "df['value_per_tonne'] = df['value_fob_aud'] / df['gross_weight_tonnes'].replace(0, np.nan)\n",
    "df['data_processed_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Years found: {sorted(df['year'].dropna().unique())}\")\n",
    "print(f\"Months found: {sorted(df['month_number'].dropna().unique())}\")\n",
    "print(f\"Month names: {df['month'].unique()}\")\n",
    "print(f\"Value per tonne calculated for {df['value_per_tonne'].notna().sum():,} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final duplicate check and removal (after all transformations)\n",
    "print(\"\\n=== FINAL DUPLICATE CHECK ===\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Found {duplicates:,} duplicate rows\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    # Show some duplicate rows as examples\n",
    "    print(\"\\nExample of duplicate rows:\")\n",
    "    duplicate_mask = df.duplicated(keep=False)\n",
    "    sample_duplicates = df[duplicate_mask].head(4).sort_values(by=['month', 'product_description'])\n",
    "    print(sample_duplicates[['month', 'year', 'product_description', 'country_of_destination', \n",
    "                              'value_fob_aud', 'gross_weight_tonnes']].to_string())\n",
    "    \n",
    "    # Remove duplicates\n",
    "    print(f\"\\nRemoving {duplicates:,} duplicate rows...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"âœ… Duplicates removed! New shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"âœ… No duplicates found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Save Cleaned Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITC CODE MAPPING TO PRODUCT DESCRIPTIONS\n",
    "print(\"=== SITC CODE MAPPING ===\")\n",
    "\n",
    "# Import SITC mapping from external file\n",
    "from sitc_mapping import map_sitc_to_product, get_unclassified_patterns\n",
    "\n",
    "# Apply SITC mapping to unclassified products\n",
    "print(\"Mapping SITC codes to product descriptions...\")\n",
    "\n",
    "# Count unclassified products before mapping\n",
    "unclassified_patterns = '|'.join(get_unclassified_patterns())\n",
    "unclassified_before = df['product_description'].str.contains(unclassified_patterns, case=False, na=False).sum()\n",
    "print(f\"Unclassified products before mapping: {unclassified_before:,}\")\n",
    "\n",
    "# Create new product description based on SITC codes for unclassified products\n",
    "mask_unclassified = df['product_description'].str.contains(unclassified_patterns, case=False, na=False)\n",
    "df.loc[mask_unclassified, 'product_description'] = df.loc[mask_unclassified, 'prod_descpt_code'].apply(map_sitc_to_product)\n",
    "\n",
    "# Count unclassified products after mapping\n",
    "unclassified_after = df['product_description'].str.contains(unclassified_patterns, case=False, na=False).sum()\n",
    "print(f\"Unclassified products after mapping: {unclassified_after:,}\")\n",
    "print(f\"Products successfully mapped: {unclassified_before - unclassified_after:,}\")\n",
    "\n",
    "print(\"SITC code mapping completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTRY CODE MAPPING TO COUNTRY NAMES\n",
    "print(\"\\n=== COUNTRY CODE MAPPING ===\")\n",
    "\n",
    "# Import country mapping from external file\n",
    "from country_mapping import map_country_code_to_name, get_problematic_country_patterns\n",
    "\n",
    "# First, let's check what values are actually in the country_of_destination column\n",
    "print(\"Checking country_of_destination column values...\")\n",
    "print(\"\\nUnique values in country_of_destination column:\")\n",
    "country_values = df['country_of_destination'].value_counts()\n",
    "print(f\"Total unique countries: {len(country_values)}\")\n",
    "print(\"\\nTop 20 countries:\")\n",
    "print(country_values.head(20))\n",
    "\n",
    "print(\"\\nChecking for problematic entries...\")\n",
    "problematic_patterns = get_problematic_country_patterns()\n",
    "problematic_countries = []\n",
    "\n",
    "for pattern in problematic_patterns:\n",
    "    matches = df['country_of_destination'].str.contains(pattern, case=False, na=False)\n",
    "    if matches.any():\n",
    "        problematic_countries.extend(df[matches]['country_of_destination'].unique())\n",
    "\n",
    "if problematic_countries:\n",
    "    print(f\"\\nFound problematic country entries:\")\n",
    "    for country in set(problematic_countries):\n",
    "        count = (df['country_of_destination'] == country).sum()\n",
    "        print(f\"  '{country}': {count:,} records\")\n",
    "else:\n",
    "    print(\"\\nNo obvious problematic entries found.\")\n",
    "\n",
    "# Apply country code mapping ONLY to truly problematic country names\n",
    "print(\"\\nMapping country codes to country names...\")\n",
    "\n",
    "# Import the new function\n",
    "from country_mapping import is_problematic_country_name\n",
    "\n",
    "# Count truly problematic entries before mapping\n",
    "problematic_mask = df['country_of_destination'].apply(is_problematic_country_name)\n",
    "missing_countries_before = problematic_mask.sum()\n",
    "print(f\"Truly problematic country details before mapping: {missing_countries_before:,}\")\n",
    "\n",
    "# Only map entries that are actually problematic\n",
    "if missing_countries_before > 0:\n",
    "    df.loc[problematic_mask, 'country_of_destination'] = df.loc[problematic_mask, 'country_of_destination_code'].apply(map_country_code_to_name)\n",
    "\n",
    "# Count problematic entries after mapping\n",
    "problematic_mask_after = df['country_of_destination'].apply(is_problematic_country_name)\n",
    "missing_countries_after = problematic_mask_after.sum()\n",
    "print(f\"Truly problematic country details after mapping: {missing_countries_after:,}\")\n",
    "print(f\"Countries successfully mapped: {missing_countries_before - missing_countries_after:,}\")\n",
    "\n",
    "# Show examples of what was mapped\n",
    "if missing_countries_before > missing_countries_after:\n",
    "    print(f\"\\nExamples of successful mappings:\")\n",
    "    mapped_entries = df[problematic_mask & ~problematic_mask_after].head(5)\n",
    "    for _, row in mapped_entries.iterrows():\n",
    "        print(f\"  Code: {row['country_of_destination_code']} â†’ Country: {row['country_of_destination']}\")\n",
    "\n",
    "print(\" Smart country code mapping completed!\")\n",
    "\n",
    "# Show some examples of the mappings\n",
    "print(\"\\n=== MAPPING EXAMPLES ===\")\n",
    "print(\"Sample SITC code mappings:\")\n",
    "sample_sitc = df[df['prod_descpt_code'].notna()]['prod_descpt_code'].head(5)\n",
    "for code in sample_sitc:\n",
    "    mapped = map_sitc_to_product(code)\n",
    "    print(f\"  {code} â†’ {mapped}\")\n",
    "\n",
    "print(\"\\nSample country code mappings:\")\n",
    "sample_countries = df[df['country_of_destination_code'].notna()]['country_of_destination_code'].head(5)\n",
    "for code in sample_countries:\n",
    "    mapped = map_country_code_to_name(code)\n",
    "    print(f\"  {code} â†’ {mapped}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset with SITC and country mappings\n",
    "import os\n",
    "\n",
    "output_file = 'data/exports_cleaned.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\" Cleaned data saved to: {output_file}\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Final Summary\n",
    "print(f\"\\n DATA CLEANING COMPLETED WITH MAPPINGS!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\" Records: {len(df):,}\")\n",
    "print(f\" Countries: {df['country_of_destination'].nunique()}\")\n",
    "print(f\" Products: {df['product_description'].nunique()}\")\n",
    "print(f\" Total Value: AUD ${df['value_fob_aud'].sum():,.2f}\")\n",
    "print(f\"Total Weight: {df['gross_weight_tonnes'].sum():,.2f} tonnes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what products have SITC code 98888\n",
    "print(\"=== PRODUCTS WITH SITC CODE 98888 ===\")\n",
    "\n",
    "# Look at the original product descriptions for records with SITC code 98888\n",
    "sitc_98888_records = df[df['prod_descpt_code'] == 98888]\n",
    "\n",
    "if len(sitc_98888_records) > 0:\n",
    "    print(f\"Found {len(sitc_98888_records):,} records with SITC code 98888\")\n",
    "    print(\"\\nOriginal product descriptions:\")\n",
    "    original_descriptions = sitc_98888_records['product_description'].value_counts()\n",
    "    print(original_descriptions)\n",
    "    \n",
    "    print(f\"\\nTotal value for SITC 98888: ${sitc_98888_records['value_fob_aud'].sum()/1e9:.2f}B\")\n",
    "    print(f\"Total weight for SITC 98888: {sitc_98888_records['gross_weight_tonnes'].sum()/1e6:.2f}M tonnes\")\n",
    "    \n",
    "    print(\"\\nTop 5 countries for SITC 98888:\")\n",
    "    country_distribution = sitc_98888_records.groupby('country_of_destination')['value_fob_aud'].sum().sort_values(ascending=False).head()\n",
    "    for country, value in country_distribution.items():\n",
    "        print(f\"  {country}: ${value/1e9:.2f}B\")\n",
    "else:\n",
    "    print(\"No records found with SITC code 98888\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX COUNTRY CODE ISSUES\n",
    "print(\"=== FIXING COUNTRY CODE ISSUES ===\")\n",
    "\n",
    "# Check for \"Country Code XXX\" entries\n",
    "country_code_entries = df[df['country_of_destination'].str.contains('Country Code', na=False)]\n",
    "print(f\"Found {len(country_code_entries):,} records with 'Country Code XXX' entries\")\n",
    "\n",
    "if len(country_code_entries) > 0:\n",
    "    print(\"\\nTop 10 Country Code entries:\")\n",
    "    country_code_counts = country_code_entries['country_of_destination'].value_counts().head(10)\n",
    "    for country_code, count in country_code_counts.items():\n",
    "        print(f\"  {country_code}: {count:,} records\")\n",
    "    \n",
    "    # Fix the country codes by extracting the code and mapping it properly\n",
    "    print(\"\\nFixing Country Code entries...\")\n",
    "    mask = df['country_of_destination'].str.contains('Country Code', na=False)\n",
    "    \n",
    "    # Extract the country code (remove \"Country Code \" prefix)\n",
    "    df.loc[mask, 'country_of_destination'] = df.loc[mask, 'country_of_destination'].str.replace('Country Code ', '')\n",
    "    \n",
    "    # Now apply proper mapping using the updated function\n",
    "    df.loc[mask, 'country_of_destination'] = df.loc[mask, 'country_of_destination'].apply(map_country_code_to_name)\n",
    "    \n",
    "    print(\" Country Code entries fixed!\")\n",
    "    \n",
    "    # Verify the fix\n",
    "    remaining_country_codes = df[df['country_of_destination'].str.contains('Country Code', na=False)]\n",
    "    print(f\"Remaining 'Country Code XXX' entries: {len(remaining_country_codes):,}\")\n",
    "    \n",
    "    if len(remaining_country_codes) == 0:\n",
    "        print(\"All Country Code entries have been successfully mapped!\")\n",
    "    else:\n",
    "        print(\"\\nRemaining entries:\")\n",
    "        for country_code, count in remaining_country_codes['country_of_destination'].value_counts().items():\n",
    "            print(f\"  {country_code}: {count:,} records\")\n",
    "else:\n",
    "    print(\"No 'Country Code XXX' entries found!\")\n",
    "\n",
    "print(\"\\nCountry code fixing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE CLEANED DATA WITH FIXED COUNTRY CODES\n",
    "print(\"=== SAVING CLEANED DATA ===\")\n",
    "\n",
    "# Save the cleaned data with fixed country codes\n",
    "output_file = 'data/exports_cleaned.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to: {output_file}\")\n",
    "print(f\"Total records saved: {len(df):,}\")\n",
    "print(f\"Total unique countries: {df['country_of_destination'].nunique():,}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
